# 1. LLM 설계를 위한 기초 원리 탐구

## 서론: 좋은 신경망 아키텍처란?
"좋은" 신경망 아키텍처는 단순히 정확도와 같은 단일 지표로 정의되지 않는다. 이는 본질적으로 다중 최적화 문제이다.

- 표현력: 복잡한 함수를 표현할 수 있는 능력
- 학습 용이성: 최적화를 통해 좋은 함수를 쉽게 찾을 수 있어야 함
- 확장성: 데이터, 파라미터, 컴퓨팅 자원을 증가시켰을 때 예측 가능한 성능 향상을 얻을 수 있어야 함
- 효율성: 학습과 추론에서 연산 및 메모리 효율이 좋아야 함

# 신경망 아키텍처의 기본 구성요소
## 뉴런: 생물학에서 추상화까지
신경망의 개념적 출발점은 뇌와 뉴런을 느슨하게 모델링한 인공 뉴런(neuron)이다. 
현대 딥러닝에서 사용되는 뉴런은 고도로 추상화된 계산 단위이다. 

## 활성화: 비선형성의 원친


## 아키텍처 설계의 핵심 trade-off

- 성능: 모델이 주어진 태스크를 얼마나 잘 수행하는가?
- 효율성: 모델이 주어진 성능을 달성하기 위해 얼마나 적은 자원을 사용하는가?
- 확장성: 모델의 크기나 데이터의 양이 증가할 때, 성능과 효율성이 어떻게 변화하는가?

|    |  목표 |  전략  | trade-off |
|---|---|---|---|
|성능| 모델의 표현력을 극대화하여 데이터의 복잡한 패턴을 학습하고, 높은 정확도를 달성하는 것 | 모델의 깊이(Depth)와 너비(Width)를 늘리고, 더 복잡한 연산 모듈을 추가 | 효율성과 확장성의 희생. 모델이 커지고 느려짐 |
|효율성| |파라미터 공유, 모델 경량화, 연산 최적화 | 모델의 표현력을 저하시켜 모델 성능 하락을 유발| 
|확장성|모델/데이터가 커짐에 따라 성능이 예측 가능하게 향상(Scaling Law)되고, 학습이 안정적으로 유지되는 것|구조의 단순성, 안정적인 학습을 위한 장치(예: 정규화, 잔차 연결), 병렬 처리에 용이한 구조| 특정 태스크에서 최고의 성능을 내지 못할 수 있음|
