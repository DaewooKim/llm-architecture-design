# 1. LLM 설계를 위한 기초 원리 탐구

## 서론: 좋은 신경망 아키텍처란?
"좋은" 신경망 아키텍처는 단순히 정확도와 같은 단일 지표로 정의되지 않는다. 이는 본질적으로 다중 최적화 문제이다.

- 표현력: 복잡한 함수를 표현할 수 있는 능력
- 학습 용이성: 최적화를 통해 좋은 함수를 쉽게 찾을 수 있어야 함
- 확장성: 데이터, 파라미터, 컴퓨팅 자원을 증가시켰을 때 예측 가능한 성능 향상을 얻을 수 있어야 함
- 효율성: 학습과 추론에서 연산 및 메모리 효율이 좋아야 함

# 신경망 아키텍처의 기본 구성요소
## 뉴런: 생물학에서 추상화까지
신경망의 개념적 출발점은 뇌와 뉴런을 느슨하게 모델링한 인공 뉴런(neuron)이다. 
현대 딥러닝에서 사용되는 뉴런은 고도로 추상화된 계산 단위이다. 

## 활성화: 비선형성의 원친


# 아키텍처 설계의 핵심 trade-off

- 성능: 모델이 주어진 태스크를 얼마나 잘 수행하는가?
- 효율성: 모델이 주어진 성능을 달성하기 위해 얼마나 적은 자원을 사용하는가?
- 확장성: 모델의 크기나 데이터의 양이 증가할 때, 성능과 효율성이 어떻게 변화하는가?

|    |  목표 |  전략  | trade-off |
|---|---|---|---|
|성능| 모델의 표현력을 극대화하여 데이터의 복잡한 패턴을 학습하고, 높은 정확도를 달성하는 것 | 모델의 깊이(Depth)와 너비(Width)를 늘리고, 더 복잡한 연산 모듈을 추가 | 효율성과 확장성의 희생. 모델이 커지고 느려짐 |
|효율성| |파라미터 공유, 모델 경량화, 연산 최적화 | 모델의 표현력을 저하시켜 모델 성능 하락을 유발| 
|확장성|모델/데이터가 커짐에 따라 성능이 예측 가능하게 향상(Scaling Law)되고, 학습이 안정적으로 유지되는 것|구조의 단순성, 안정적인 학습을 위한 장치(예: 정규화, 잔차 연결), 병렬 처리에 용이한 구조| 특정 태스크에서 최고의 성능을 내지 못할 수 있음|

# Inductive Bias(귀납적 편향)
## 일반화
모든 ML 알고리즘의 핵심에는 일반화(generalization)라는 목표가 있다. 일반화는 데이터에서 학습한 패턴을 바탕으로 이전에 보지 못한 새로운 데이터에 대해서도 정확한 예측을 수행하는 능력이다. 이 일반화를 가능하게 하는 것이 바로 Inductive Bias이다.
## Inductive Bias의 정의
**Inductive Bias**은 학습 알고리즘이 훈련 데이터에서 관찰된 것을 넘어 일반화하기 위해 사용하는 모델 구조에 내재된 가정의 집합이다. 즉 네트워크 구조 자체가 "세상은 이럴 것"이라는 어떤 전제를 암묵적으로 가지게 만드는 특성이 **Inductive Bias**이다. 귀납적 편향은 강한 편향에서 약한 편향까지의 스펙트럼을 가진다. Inductive bias는 모델의 학습 효율, 일반화 성능, 데이터 요구량에 큰 영향을 미친다.

- Inductive bias가 강할수록
  - 적은 데이터로도 구조적 정보를 빠르게 습득(예: CNN, SSM)
- Inductive bias가 약할수록:
  - 더 많은 데이터 필요, 유연성↑ (예: Transformer, MLP)
 
| 모델 | Inductive Bias | 데이터 요구량 | 주요 장점 | 주요 한계 |
|---|---|---|---|---|
|MLP|없음|매우 높음|범용, 어떤 구조든 적용|비효율, 구조정보 활용 불가|
|CNN|지역성,가중치 공유,이동 불변|중간|이미지/신호 패턴 효율적 학습|긴 거리 상호작용 한계|
|Transformer|전역 의존성, 순서+전역 상호작용|매우 높음|장기 의존성, 유연한 입력|구조적 Inductive bias 약함|
|SSM|상태 천이, 시계열/마르코프 가정|낮음-중간|시계열/동적 시스템 최적화|비시계열 데이터 부적합|




