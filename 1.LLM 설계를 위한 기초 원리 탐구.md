# 1. LLM 설계를 위한 기초 원리 탐구

## 서론: 좋은 신경망 아키텍처란?
"좋은" 신경망 아키텍처는 단순히 정확도와 같은 단일 지표로 정의되지 않는다. 이는 본질적으로 다중 최적화 문제이다.

- 표현력: 복잡한 함수를 표현할 수 있는 능력
- 학습 용이성: 최적화를 통해 좋은 함수를 쉽게 찾을 수 있어야 함
- 확장성: 데이터, 파라미터, 컴퓨팅 자원을 증가시켰을 때 예측 가능한 성능 향상을 얻을 수 있어야 함
- 효율성: 학습과 추론에서 연산 및 메모리 효율이 좋아야 함

# 신경망 아키텍처의 기본 구성요소
## 뉴런: 생물학에서 추상화까지
신경망의 개념적 출발점은 뇌와 뉴런을 느슨하게 모델링한 인공 뉴런(neuron)이다. 
현대 딥러닝에서 사용되는 뉴런은 고도로 추상화된 계산 단위이다. 

## 활성화: 비선형성의 원친


# 아키텍처 설계의 핵심 trade-off

- 성능: 모델이 주어진 태스크를 얼마나 잘 수행하는가?
- 효율성: 모델이 주어진 성능을 달성하기 위해 얼마나 적은 자원을 사용하는가?
- 확장성: 모델의 크기나 데이터의 양이 증가할 때, 성능과 효율성이 어떻게 변화하는가?

|    |  목표 |  전략  | trade-off |
|---|---|---|---|
|성능| 모델의 표현력을 극대화하여 데이터의 복잡한 패턴을 학습하고, 높은 정확도를 달성하는 것 | 모델의 깊이(Depth)와 너비(Width)를 늘리고, 더 복잡한 연산 모듈을 추가 | 효율성과 확장성의 희생. 모델이 커지고 느려짐 |
|효율성| |파라미터 공유, 모델 경량화, 연산 최적화 | 모델의 표현력을 저하시켜 모델 성능 하락을 유발| 
|확장성|모델/데이터가 커짐에 따라 성능이 예측 가능하게 향상(Scaling Law)되고, 학습이 안정적으로 유지되는 것|구조의 단순성, 안정적인 학습을 위한 장치(예: 정규화, 잔차 연결), 병렬 처리에 용이한 구조| 특정 태스크에서 최고의 성능을 내지 못할 수 있음|

# Inductive Bias(귀납적 편향)
## 일반화
모든 ML 알고리즘의 핵심에는 일반화(generalization)라는 목표가 있다. 일반화는 데이터에서 학습한 패턴을 바탕으로 이전에 보지 못한 새로운 데이터에 대해서도 정확한 예측을 수행하는 능력이다. 이 일반화를 가능하게 하는 것이 바로 Inductive Bias이다.
## Inductive Bias의 정의
**Inductive Bias**은 학습 알고리즘이 훈련 데이터에서 관찰된 것을 넘어 일반화하기 위해 사용하는 모델 구조에 내재된 가정의 집합이다. 즉 네트워크 구조 자체가 "세상은 이럴 것"이라는 어떤 전제를 암묵적으로 가지게 만드는 특성이 **Inductive Bias**이다. 귀납적 편향은 강한 편향에서 약한 편향까지의 스펙트럼을 가진다. Inductive bias는 모델의 학습 효율, 일반화 성능, 데이터 요구량에 큰 영향을 미친다.

- Inductive bias가 강할수록
  - 적은 데이터로도 구조적 정보를 빠르게 습득(예: CNN, SSM)
- Inductive bias가 약할수록:
  - 더 많은 데이터 필요, 유연성↑ (예: Transformer, MLP)
 
| 모델 | Inductive Bias | 데이터 요구량 | 주요 장점 | 주요 한계 |
|---|---|---|---|---|
|MLP|없음|매우 높음|범용, 어떤 구조든 적용|비효율, 구조정보 활용 불가|
|CNN|지역성,가중치 공유,이동 불변|중간|이미지/신호 패턴 효율적 학습|긴 거리 상호작용 한계|
|Transformer|전역 의존성, 순서+전역 상호작용|매우 높음|장기 의존성, 유연한 입력|구조적 Inductive bias 약함|
|SSM|상태 천이, 시계열/마르코프 가정|낮음-중간|시계열/동적 시스템 최적화|비시계열 데이터 부적합|

# Transformer 이전
아키텍처의 위대함은 단지 표현력에 있는 것이 아니라 경사 하강법기반 최적화의 행동 방식과 조화를 이루는 구조를 설계하여 모델이 단순하고 근본적인 구성 요소를 쉽게 학습할 수 있도록 만드는 것이 중요하다.

## RNN의 특징
- 순차 처리
- 역전파를 수행할 때, 기울기 소실 및 폭주
## CNN의 특징
- 지역성: 이미지의 한 픽셀은 주변 픽셀들과 강한 상관관계를 갖는다는 가정
- 이동 불변성: 이미지 속 객체는 위치가 바뀌어도 동일한 객체라는 가정
- MLP에 비해 파라미터 수가 작음.
- 데이터 효율적임
- 강력한 inductive bias의 한계로 자연어 처리와 같은 작업에 부적합함
## Resnet
- 네트워크가 점점 더 깊어지면 성능 저하 현상이 발생함
- Residual connection 또는 Skip connection으로 해결
- 성능 저하 문제가 해결되고, 기울기 흐림이 안정화되었으며 수백, 수천 개의 계층을 가진 뉴럴 네트워크의 학습이 가능해짐


# Transformer 혁명
Transformer 아키텍처는 점진적인 개선이 아니라, 이전 아키텍처들의 근본적인 한계를 극복하기 위한 패러다임의 전환이었다.

## 이론적 근거: 순환성의 사슬을 끊다.
Transformer의 핵심 동기는 순환성을 완전히 없애는 것이다 이러한 순차성은 병렬화를 제한하며 대규모 데이터셋에서의 학습 속도를 저해하는 주요 병목 현상으로 작용했다. Transformer는 이 순환성의 사슬을 끊고 모든 입력 토큰을 동시에 처리할 수 있는 아키텍처를 제안함으로써 이 문제를 해결하고자 했다.   

## Self-attention 메커니즘
트랜스포머의 핵심 혁신은 셀프 어텐션(self-attention) 메커니즘이다. 이는 모델이 특정 단어를 인코딩할 때 시퀀스 내의 다른 모든 단어의 중요도를 직접적으로 가중치를 두어 계산할 수 있게 한다.

- 쿼리, 키, 값 (Query, Key, Value - QKV) 모델: 이 개념은 정보 검색 분야에서 유추되었다. 시퀀스 내 각 토큰은 세 가지 벡터를 생성한다: **쿼리(Query)**는 "내가 찾고 있는 것"을, **키(Key)**는 "내가 담고 있는 정보"를, **값(Value)**은 "키와 쿼리가 일치할 때 내가 제공할 정보"를 나타낸다. 특정 토큰의 어텐션 점수는 해당 토큰의 쿼리 벡터와 시퀀스 내 다른 모든 토큰의 키 벡터 간의 내적(dot product)을 통해 계산된다.
- Sacled Dot-Product Attention
- Multi-Head Attention

## Positional Encoding의 역할
순환성을 제거함으로써 Transformer 아키텍처는 본질적으로 **순열 등변성**을 갖게 되었다. 즉, "개는 사람을 문다."와 "사람은 개를 문다."라는 두 문장은 단어의 순서만 다를 뿐, Self-Attetion 메커니즘에게는 동일하게 보인다. 서로 다른 주파수를 가진 사인(sine) 및 코사인(cosine) 함수를 사용했다. 이 선택이 우아했던 이유는 다음과 같다:   

- 상대적 위치 표현: 각 위치 인코딩은 고정된 선형 변환을 통해 다른 위치의 인코딩으로 표현될 수 있어, 모델이 상대적 위치 관계를 쉽게 학습할 수 있다.
- 일반화 능력: 학습 중에 보았던 것보다 더 긴 시퀀스 길이에 대해서도 일반화할 수 있는 능력을 제공한다.

Transfomer는 자연스러운 순차적 처리를 희생하는 대신 대규모 병렬 처리 능력을 얻는 trade-off를 나타낸다.

# 아키텍처 우수성을 위한 이론적 원리
정보 병목 원리와 귀납적 편향에 대한 심층적 분석을 통해 왜 특정 아키텍처가 다른 것들보다 더 잘 작동하는지에 대한 근본적으로 통찰해 본다.

## 정보 병목 원리
슈워츠-지브(Shwartz-Ziv)와 티슈비(Tishby)는 DNN 훈련이 두 단계(Fitting & Compression)로 구성된다는 영향력 있는 (그리고 논쟁적인) 가설을 제시했다. 

- 적합 단계(Fitting): 네트워크는 I(X;Z)(입력에 대한 정보)와 I(Z;Y)(출력에 대한 정보)를 모두 빠르게 증가시키며, 본질적으로 훈련 데이터를 암기한다.
- 압축 단계(Compression): 그 후 네트워크는 입력에 대한 불필요한 정보를 "잊기" 시작하여, $I(Z;Y)$는 높게 유지하면서 $I(X;Z)$를 감소시킨다. 이 압축 과정이 일반화의 핵심으로 추정되었다.

## Transformer의 Inductive Bias
- 순열 대칭성 (Permutation Symmetry): 더 최근의 심오한 이론적 관점은 무한 너비(infinite-width) 극한에서 학습된 위치 인코딩을 가진 트랜스포머가 순열 대칭적인 함수를 학습하도록 편향되어 있다고 주장한다. 이는 시퀀스 내 토큰들의 순서를 바꾸어도 함수의 출력이 특정 대칭성을 유지하는 경향이 있다는 의미이다.   
- 대칭군의 표현론 (Representation Theory of the Symmetric Group): 이 편향을 분석하기 위한 도구로 대칭군의 표현론이 사용된다. 함수는 순열군의 기약 표현(irreducible representations, irreps)으로 분해될 수 있다. 더 낮은 차원의 기약 표현에 해당하는 함수는 "더 대칭적"이며, 트랜스포머에 의해 더 쉽게 학습된다(즉, 더 적은 샘플을 필요로 한다). 이는 함수의 대칭 속성에 기반한 학습 용이성에 대한 정량적인 스케일링 법칙을 제공한다.   
- 시사점: 이는 트랜스포머가 토큰 간의 관계가 어느 정도 순열 대칭성을 갖는 작업에 자연스럽게 강점을 보인다는 것을 시사한다. 이러한 특성은 WikiText와 같은 자연어 데이터셋에서도 경험적으로 관찰되었다.   

## 교훈
1. 모든 학습 알고리즘의 목표는 데이터의 노이즈가 아닌 진정한 기본 신호를 포착하여 일반화하는 모델을 찾는 것이다.   
2. 정보 병목 원리는 모델이 특정 입력 X에 대한 정보를 가능한 한 많이 버리고, 오직 Y를 예측하는 데 필요한 것만 유지해야 한다고 명시함으로써 이를 공식화한다. 이는 "가장 단순한 설명이 가장 좋다"는 오컴의 면도날 원칙의 한 형태이다.   
3. 트랜스포머의 귀납적 편향에 대한 연구는  이것이 어떻게 일어날 수 있는지에 대한 구체적인 메커니즘을 제공한다. 높은 수준의 순열 대칭성을 가진 함수에 편향됨으로써, 트랜스포머는 모든 가능한 시퀀스-투-시퀀스 함수 공간 내에서 "더 단순한" 함수에 대한 내재된 선호를 갖게 된다. 더 복잡하고 덜 대칭적인 함수는 학습하는 데 더 많은 데이터가 필요하다.   
4. 이는 최적화기를 위한 "최소 저항 경로"라는 아이디어와 다시 연결된다. 이러한 고차원 공간에서 경사 하강법은 "가장 쉬운" 해를 먼저 찾을 것이다. 아키텍처의 귀납적 편향은 무엇이 "쉬운지"를 정의한다. 트랜스포머에게는 대칭적인 함수가 "쉽다".
5. 따라서 이 두 이론적 기둥은 서로 다른 언어로 동일한 근본 원리를 설명하고 있다. 아키텍처적 위대함은 단순하고 일반화 가능한 해에 대한 선호를 내장하는 것에서 비롯되며, 그 선호가 정보 이론적 압축으로 설명되든 군론적 대칭성으로 설명되든 마찬가지이다.

# 결론
"위대한" 아키텍처는 고정된 청사진이 아니라, 복잡하고 다차원적인 설계 공간을 능숙하게 탐색하는 동적인 해결책이다. 이 보고서의 심층 분석을 통해, 위대함은 여러 핵심 원칙들의 교차점에서 나타난다는 것이 명확해졌다.

1. **적절한 귀납적 편향이다.** 아키텍처는 데이터와 작업의 구조에 부합하는 가정을 가져야 한다. 그것이 CNN처럼 강력하고 구체적이든, 트랜스포머처럼 약하고 일반적이든, 올바른 편향은 유한한 데이터로부터의 일반화를 가능하게 하는 필수 조건이다.

2. **효율적인 정보 흐름이다.** 아키텍처는 예측을 위한 순방향 신호와 학습을 위한 역방향 신호(기울기)가 효과적으로 전파되도록 해야 하며, 기울기 소실이나 성능 저하와 같은 문제를 피해야 한다. ResNet의 잔차 연결은 이 원칙의 대표적인 구현이다. 정보 병목 원리는 이러한 정보 흐름이 이론적으로 무엇을 달성해야 하는지에 대한 통찰을 제공한다.

3. **예측 가능한 확장성이다.** 현대 대규모 AI 개발을 지배하는 거듭제곱 법칙 동역학에 따라, 아키텍처는 규모가 커짐에 따라 성능이 안정적으로 향상되도록 설계되어야 한다. 이를 위해서는 트랜스포머가 보여주었듯이 본질적인 병렬 처리 능력이 전제되어야 한다.

4. **계산적 및 경제적 처리 가능성이다.** 아키텍처는 현실적인 계산 및 재정 예산 내에서 훈련되고 배포될 수 있을 만큼 효율적이어야 한다. 이는 수치 정밀도, MoE의 희소 활성화, SSM의 알고리즘 복잡도와 같은 현대적 고려 사항을 설계의 중심에 놓는다.

결론적으로, 위대한 아키텍처의 정의는 끊임없이 진화하는 목표이다. 우리의 이론적 이해가 깊어지고  기술적 능력이 확장됨에 따라, "위대함"의 기준은 계속해서 변할 것이다. 오늘날의 위대한 아키텍처는 내일의 표준이 될 것이며, 이러한 끊임없는 탐구와 혁신이 인공지능의 가능성의 경계를 계속해서 넓혀 나갈 것이다.   







