# Representation Learning (Fature Learning) 개념
- 원시 데이터는 그 자체로 매우 고차원적, 노이즈가 많고, 알고리즘이 직접적으로 학습하기엔 비효율적임
- 이 문제를 해결하기 위해 데이터를 의미 있는 "표현(Representation)"으로 바꾸는 과정이 필수적임
- Representation Learning(또는 Feature Learning)이란 알고리즘이 원시 데이터부터 직접 유용하고 의미 있는 특징을 **자동으로** 발견하고 학습하는 접근법임
- 이 과정의 최종 산물은 데이터의 본질적인 정보를 압축한 저차원의 dense 벡터, 즉 임베딩임
- 이러한 벡터는 인간에게는 직관적으로 해석되기 어렵지만, 여러 차원에 정보가 분산되어 있어 컴퓨터에게는 유의미한 내용을 포함함
- 사람이 정한 범주형 특징은 **one-hot 인코딩**처럼 고차원 희소 벡터로 나타남

# Representation Learning의 중요성
- Representaion Learning은 고차원 데이터를 저차원 벡터 공간에 효율적으로 압축하며, 이를 통해 모델이 보다 일반화된 특징을 학습하도록 도움
- 표현 학습이 중요한 이유는 복잡한 비정형 데이터를 다루는 현대 머신러닝의 성능 향상에 핵심적인 역할을 하기 때문임
- 사람 손으로 특징을 설계하기 어려운 이미지, 텍스트, 그래프 등의 데이터에서도 표현 학습을 통해 유용한 표현을 자동 학습할 수 있음

# 데이터 표현의 기초 원리
## 좋은 표현이란?
- 1st: **Compact**해야 함. 원본 데이터보다 훨씬 낮은 차원으로 정보를 압축할 수 있어야 함
- 2nd: 원본 데이터의 필수적인 정보는 보존하면서 관련 없는 노이즈는 폐기해야 함
- 3rd: 분류나 예측과 같은 후속 downstream task를 더 쉽게 만들어야 함

## 매니폴드 가설(The Manifold Hypothesis)
- 매니폴드 가설은 이미지와 같은 고차원의 실제 데이터가 사실은 그보다 휠씬 낮은 차원의 비선형적인 매니폴드 위에 놓여있다라고 가정한다.
- 예를들어 세상의 모든 '고양이' 이미지는 수백만 개의 픽셀로 이루어진 고차원 공간에 존재하지만, 이 점들은 무작위로 흩어져 있는 것이 아니라 '고양이'라는 개념을 정의하는 특정 구조, 즉 저차원 매니폴드를 형성한다.
- 표현 학습의 목표는 이렇게 복잡하게 얽혀 있는 매니폴드를 '펼쳐서' 데이터의 클래스들이 더 쉽게 분리될 수 있는 단순한 저차원 공간으로 매핑하는 것이다.

## 분산 표현 vs 기호 표현
- 기호 표현
  - 고전적인 인공지능에서 주로 사용된 방식
  - 예) '고양이' = [동물, 털이 있음, 다리가 4개]
- 분산 표현
  - dense 벡터(임베딩)
  - 예) one-hot encoding : 단어 하나를 하나의 차원에서만 1의 값을 갖는 희소 벡터로 표현. 이는 비효율적이고 단어 간의 의미 관계를 포착할 수 있음
  - 예) dense 벡터: A=[0.5, 0.3, 0.0, 0.5], dense 벡터를 사용하여 단어 간의 미묘하고 복잡한 관계를 효과적으로 포착할 수 있음 

 # 신경망을 이용한 표현 학습 기법
 ## 오토인코더(Autoencoder)
 ### 핵심 아키텍처
 - 입력 데이터를 받아 압축한 후, 다시 원본 데이터로 복원하는 방법을 학습하는 신경망 구조로 대표적인 비지도 표현 학습 모델임
 - 오토인코더는 데이터의 중요한 특질을 손실없이 압축하는 표현을 학습함
 - 세가지 주요 구성 요소
   - Encoder: 입력 데이터를 받아 더 낮은 차원의 latent space로 압축하는 역할을 함
   - Bottleneck: 인코딩된 표현이 위치하는 가장 압축된 형태의 레이어. 이 잠재 표현이 바로 우리가 얻고자 하는 데이터의 새로운 '표현'임
   - Decoder: 병목 계층의 압축된 표현을 다시 입력 데이터와 동일한 차원으로 복원하는 역할
- 오토인코더의 학습 목표
  - 오토인코더 구조의 특징은 입력과 출력이 동일하다는 것임
  - 네트워크가 자기 자신을 복제하도록 학습하는 과정에서 불필요한 정보는 버리고 핵심 패턴만 latent 벡터에 저장하려는 압력이 생김
  - 입력 데이터와 디코더가 재구성한 출력 데이터 간의 **재구성 손실(reconstruction loss)**을 최소화하는 것
  - 손실 함수: MSE,Cross-Entropy
  - 모델은 이 손실을 줄이는 과정에서 단순히 입력을 복사하는 것이 아니라, 데이터의 가장 중요하고 본질적인 특징을 잠재 공간에 인코딩하는 법을 배움
- 오토인코더의 활용 사례
  - 차원 축소 및 데이터 압축, 잡음 제거, 이상치 탐지
  - Denoising Autoencoder: 노이즈가 섞인 이미지를 입력으로 깨끗한 이미지를 출력하도록 오토인코더를 학습시키면 노이즈 제거에 사용할 수 있음
  - Anomaly Detection(이상감지): 정상적인 데이터로 학습된 오토인코더는 학습 시 보지 못한 이상치 데이터를 넣으면 복원 오류가 커지므로 이상 신호를 감지할 수 있음
 
## CNN

