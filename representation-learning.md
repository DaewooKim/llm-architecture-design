# 서론: Representation Learning (Fature Learning)
- 원시 데이터는 그 자체로 매우 고차원적, 노이즈가 많고, 알고리즘이 직접적으로 학습하기엔 비효율적임
- 이 문제를 해결하기 위해 데이터를 의미 있는 "표현(Representation)"으로 바꾸는 과정이 필수적임
- Representation Learning 또는 Feature Learning이란 알고리즘이 원시 데이터부터 직접 유용하고 의미 있는 특징을 **자동으로** 발견하고 학습하는 접근법이다.
- 이 과정의 최종 산물은 데이터의 본질적인 정보를 압축한 저차원의 dense 벡터, 즉 임베딩이다.

# 데이터 표현의 기초 원리
## 좋은 표현이란?
- 1st: **Compact**해야 함. 원본 데이터보다 훨씬 낮은 차원으로 정보를 압축할 수 있어야 함
- 2nd: 원본 데이터의 필수적인 정보는 보존하면서 관련 없는 노이즈는 폐기해야 함
- 3rd: 분류나 예측과 같은 후속 downstream task를 더 쉽게 만들어야 함

## 매니폴드 가설(The Manifold Hypothesis)
- 매니폴드 가설은 이미지와 같은 고차원의 실제 데이터가 사실은 그보다 휠씬 낮은 차원의 비선형적인 매니폴드 위에 놓여있다라고 가정한다.
- 예를들어 세상의 모든 '고양이' 이미지는 수백만 개의 픽셀로 이루어진 고차원 공간에 존재하지만, 이 점들은 무작위로 흩어져 있는 것이 아니라 '고양이'라는 개념을 정의하는 특정 구조, 즉 저차원 매니폴드를 형성한다.
- 표현 학습의 목표는 이렇게 복잡하게 얽혀 있는 매니폴드를 '펼쳐서' 데이터의 클래스들이 더 쉽게 분리될 수 있는 단순한 저차원 공간으로 매핑하는 것이다.

## 분산 표현 vs 기호 표현
- 기호 표현
  - 고전적인 인공지능에서 주로 사용된 방식
  - 예) '고양이' = [동물, 털이 있음, 다리가 4개]
- 분산 표현
  - dense 벡터(임베딩)
  - 예) one-hot encoding : 단어 하나를 하나의 차원에서만 1의 값을 갖는 희소 벡터로 표현. 이는 비효율적이고 단어 간의 의미 관계를 포착할 수 있음
  - 예) dense 벡터: A=[0.5, 0.3, 0.0, 0.5], dense 벡터를 사용하여 단어 간의 미묘하고 복잡한 관계를 효과적으로 포착할 수 있음 

 # 신경망을 이용한 비지도 표현 학습
 ## 오토인코더
 ### 핵심 아키텍처
 - 입력 데이터를 받아 압축한 후, 다시 원본 데이터로 복원하도록 학습되는 특별한 구조의 신경망
 - 세가지 주요 구성 요소
   - Encoder: 입력 데이터를 받아 더 낮은 차원의 latent space로 압축하는 역할을 함
   - Bottleneck: 인코딩된 표현이 위치하는 가장 압축된 형태의 레이어. 이 잠재 표현이 바로 우리가 얻고자 하는 데이터의 새로운 '표현'임
   - Decoder: 병목 계층의 압축된 표현을 다시 입력 데이터와 동일한 차원으로 복원하는 역할
- 오토인코더의 학습 목표
  - 입력 데이터와 디코더가 재구성한 출력 데이터 간의 **재구성 손실(reconstruction loss)**을 최소화하는 것
  - 손실 함수: MSE,Cross-Entropy
  - 모델은 이 손실을 줄이는 과정에서 단순히 입력을 복사하는 것이 아니라, 데이터의 가장 중요하고 본질적인 특징을 잠재 공간에 인코딩하는 법을 배움 
